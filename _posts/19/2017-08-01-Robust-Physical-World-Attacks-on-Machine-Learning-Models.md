---
layout: post
title: Robust Physical-World Attacks on Machine Learning Models
posturl: https://arxiv.org/pdf/1707.08945v1.pdf
tags:
- Machine Learning
- Attacks
- Research
---

## {{ page.title }}
<span style="font-size: 0.8em; line-height: 0.8em">Posted on {{ page.date | date: "%Y %B %-d" }} by <a href="https://twitter.com/polgarp">polgarp</a> &middot; Tagged in {{ page.tags | array_to_sentence_string }}</span>  
<span style="font-size: 0.8em; line-height: 0.8em">Original link: <{{page.posturl}}></span>

It's interesting to see more and more research published on how to fool machine learning systems, vision based in particular. I can see designers on both sides, on one had as more and more ML tech is deployed to follow people without their informed consent there is a need for systems enabling users to hide. On the other hand systems designed should protect well-intentioned users from harm coming from attackers.

<!--more-->
<a href="http://twitter.com/share?text={{page.title}}&url={{site.site_baseurl}}{{page.url}}&via=polgarp" target="_blank">Tweet this post</a> &#x25cf; <a href="{{ site.baseurl }}">Back to home</a>
